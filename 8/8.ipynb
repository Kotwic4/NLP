{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "The task concentrates on content-based text the classification.\n",
    "\n",
    "\n",
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the set of bills into two exclusive sets:\n",
    "   1. the set of bills amending other bills (their title starts with `o zmianie ustawy`),\n",
    "   1. the set of bills not amending other bills.\n",
    "\n",
    "### Change the contents of the bill by removing the date of publication and the title (so the words `o zmianie ustawy` are removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import os\n",
    "import requests\n",
    "from collections import Counter\n",
    "from operator import add\n",
    "import functools\n",
    "import random\n",
    "import math\n",
    "import fastText\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_names():\n",
    "    path = '../ustawy'\n",
    "    absolute_path = os.path.realpath(path) + \"\\\\\"\n",
    "    return [(absolute_path + filename, filename) for filename in os.listdir(path)]\n",
    "\n",
    "def get_file_text_raw(filename):\n",
    "    with open(filename, 'r', encoding=\"utf8\") as content_file:\n",
    "        return regex.sub(r\"\\n\\s*\\n\", \"\\n\", content_file.read()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found for: 1996_400.txt\n",
      "713 466\n"
     ]
    }
   ],
   "source": [
    "def split_by_title(text):\n",
    "    search = regex.search(r'((Art.)|(Rozdział))(\\s+1)',text)\n",
    "    if search is None:\n",
    "        return None, None\n",
    "    return (text[:search.start()], text[search.start():])\n",
    "\n",
    "def split_by_changing(text):\n",
    "    search = regex.search(r'(zmianie|zmieniająca)(.|\\n)*(ustaw|ustawy)',text)\n",
    "    if search is None:\n",
    "        return None\n",
    "    return text[:search.start()]\n",
    "\n",
    "possitive = []\n",
    "negative = []\n",
    "\n",
    "for (path, filename) in files_names():\n",
    "    text = get_file_text_raw(path)\n",
    "    title, body = split_by_title(text)\n",
    "    if title is None:\n",
    "        print(\"Not found for: \" + filename)\n",
    "    else:\n",
    "        result = split_by_changing(title)\n",
    "        if result is None:\n",
    "            negative.append((body,'__label__normal'))\n",
    "        else:\n",
    "            possitive.append((body,'__label__changing'))\n",
    "print(len(possitive), len(negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the sets of documents into the following groups by randomly selecting the documents:\n",
    "   1. 60% training\n",
    "   1. 20% validation\n",
    "   1. 20% testing\n",
    "   \n",
    "### Do not change these groups during the following experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_possitive = possitive[:]\n",
    "random_negative = negative[:]\n",
    "random.shuffle(random_possitive)\n",
    "random.shuffle(random_negative)\n",
    "\n",
    "possitive_training_number = math.floor(len(random_possitive)*0.6)\n",
    "possitive_validation_number = math.floor(len(random_possitive)*0.8)\n",
    "\n",
    "negative_training_number = math.floor(len(random_negative)*0.6)\n",
    "negative_validation_number = math.floor(len(random_negative)*0.8)\n",
    "\n",
    "\n",
    "training_positive = random_possitive[:possitive_training_number]\n",
    "training_negative = random_negative[:negative_training_number]\n",
    "training_set = training_positive[:] + training_negative[:]\n",
    "random.shuffle(training_set)\n",
    "\n",
    "validation_positive = random_possitive[possitive_training_number:possitive_validation_number]\n",
    "validation_negative = random_negative[negative_training_number:negative_validation_number]\n",
    "validation_set = validation_positive[:] + validation_negative[:]\n",
    "random.shuffle(validation_set)\n",
    "\n",
    "testing_positive = random_possitive[possitive_validation_number:]\n",
    "testing_negative = random_negative[negative_validation_number:]\n",
    "testing_set = testing_positive[:] + testing_negative[:]\n",
    "random.shuffle(testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the following variants of the documents:\n",
    "   1. full text of the document\n",
    "   1. randomly selected 10% of the lines of the document\n",
    "   1. randomly selected 10 lines of the document\n",
    "   1. randomly selected 1 line of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(text, number):\n",
    "    lines = text.split('\\n')\n",
    "    if len(lines) < number:\n",
    "        return lines\n",
    "    return random.sample(lines,number)\n",
    "\n",
    "def get_line_percetage(text, number):\n",
    "    lines = text.split('\\n')\n",
    "    return random.sample(lines, math.ceil(len(lines)*number))\n",
    "\n",
    "def prepare_a(data_set):\n",
    "    return [(\" \".join(text.split('\\n')), result) for (text, result) in data_set]\n",
    "\n",
    "def prepare_b(data_set):\n",
    "    return [(\" \".join(get_line_percetage(text, 0.1)), result) for (text, result) in data_set]\n",
    "\n",
    "def prepare_c(data_set):\n",
    "    return [(\" \".join(get_lines(text, 10)), result) for (text, result) in data_set]\n",
    "\n",
    "def prepare_d(data_set):\n",
    "    return [(\" \".join(get_lines(text, 1)), result) for (text, result) in data_set]\n",
    "\n",
    "training_set_a = prepare_a(training_set)\n",
    "training_set_b = prepare_b(training_set)\n",
    "training_set_c = prepare_c(training_set)\n",
    "training_set_d = prepare_d(training_set)\n",
    "\n",
    "validation_set_a = prepare_a(validation_set)\n",
    "validation_set_b = prepare_b(validation_set)\n",
    "validation_set_c = prepare_c(validation_set)\n",
    "validation_set_d = prepare_d(validation_set)\n",
    "\n",
    "testing_set_a = prepare_a(testing_set)\n",
    "testing_set_b = prepare_b(testing_set)\n",
    "testing_set_c = prepare_c(testing_set)\n",
    "testing_set_d = prepare_d(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fast_text_file(data_set, label, set_type):\n",
    "    with open('{}-{}.txt'.format(label, set_type), 'w', encoding='utf8') as f:\n",
    "        f.write('\\n'.join(['{} {}'.format(y,x) for (x,y) in data_set]))\n",
    "\n",
    "def prepare_fast_text(training_set, validation_set, testing_set, label):\n",
    "    prepare_fast_text_file(training_set, label, \"training\")\n",
    "    prepare_fast_text_file(validation_set, label, \"validation\")\n",
    "    prepare_fast_text_file(testing_set, label, \"testing\")\n",
    "\n",
    "prepare_fast_text(training_set_a, validation_set_a, testing_set_a, 'a')\n",
    "prepare_fast_text(training_set_b, validation_set_b, testing_set_b, 'b')\n",
    "prepare_fast_text(training_set_c, validation_set_c, testing_set_c, 'c')\n",
    "prepare_fast_text(training_set_d, validation_set_d, testing_set_d, 'd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the following classifiers on the documents:\n",
    "\n",
    "   1. SVM with TF•IDF\n",
    "   1. Fasttext\n",
    "   1. Flair with Polish language model\n",
    "   \n",
    "### Report Precision, Recall and F1 for each variant of the experiment (12 variants altogether)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(results, test_set):\n",
    "    labels = [y for (x,y) in test_set]\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    for (predict, label) in zip(results, labels):\n",
    "        if label == '__label__changing':\n",
    "            if label == predict:\n",
    "                tp = tp + 1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if label == predict:\n",
    "                tn = tn + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "                \n",
    "    recall = tp/(tp+fn)\n",
    "    precision = tp/(tp + fp)\n",
    "    f1 = (2 * precision * recall)/(precision + recall)\n",
    "    return (recall, precision, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metric(label, metric):\n",
    "    print(label)\n",
    "    print('recall: {},\\nprecision: {},\\nf1: {}'.format(metric[0], metric[1], metric[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_tf_idf(training_set, test_set):\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('clf', OneVsRestClassifier(LinearSVC())),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "        \"clf__estimator__class_weight\": ['balanced', None],\n",
    "    }\n",
    "    grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=8, verbose=3)\n",
    "    grid_search_tune.fit([x for (x,y) in training_set], [y for (x,y) in training_set])\n",
    "    return grid_search_tune.best_estimator_.predict([x for (x,y) in test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_text(testing_set, label):\n",
    "    classifier = fastText.train_supervised('{}-training.txt'.format(label))\n",
    "    result = classifier.test_label('{}-testing.txt'.format(label))['__label__changing']\n",
    "    return [classifier.predict(x)[0][0] for (x,y) in testing_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(training_set,testing_set, label):\n",
    "    svm_tf_idf_metric = metric(svm_tf_idf(training_set, testing_set),testing_set)\n",
    "    fast_text_metric = metric(fast_text(testing_set, label), testing_set)\n",
    "    \n",
    "    print_metric(\"svm_tf_idf\", svm_tf_idf_metric)\n",
    "    print_metric(\"fast_text\", fast_text_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   41.2s\n",
      "[Parallel(n_jobs=8)]: Done 108 out of 108 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_tf_idf\n",
      "recall: 0.965034965034965,\n",
      "precision: 0.8961038961038961,\n",
      "f1: 0.9292929292929293\n",
      "fast_text\n",
      "recall: 1.0,\n",
      "precision: 0.6111111111111112,\n",
      "f1: 0.7586206896551725\n"
     ]
    }
   ],
   "source": [
    "test(training_set_a, testing_set_a, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=8)]: Done 108 out of 108 | elapsed:   33.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_tf_idf\n",
      "recall: 0.8741258741258742,\n",
      "precision: 0.8169934640522876,\n",
      "f1: 0.8445945945945946\n",
      "fast_text\n",
      "recall: 0.972027972027972,\n",
      "precision: 0.634703196347032,\n",
      "f1: 0.7679558011049723\n"
     ]
    }
   ],
   "source": [
    "test(training_set_b, testing_set_b, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=8)]: Done 108 out of 108 | elapsed:    5.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_tf_idf\n",
      "recall: 0.8181818181818182,\n",
      "precision: 0.7905405405405406,\n",
      "f1: 0.8041237113402062\n",
      "fast_text\n",
      "recall: 1.0,\n",
      "precision: 0.6137339055793991,\n",
      "f1: 0.7606382978723404\n"
     ]
    }
   ],
   "source": [
    "test(training_set_c, testing_set_c, 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 108 out of 108 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_tf_idf\n",
      "recall: 0.7902097902097902,\n",
      "precision: 0.6848484848484848,\n",
      "f1: 0.7337662337662337\n",
      "fast_text\n",
      "recall: 1.0,\n",
      "precision: 0.6033755274261603,\n",
      "f1: 0.7526315789473684\n"
     ]
    }
   ],
   "source": [
    "test(training_set_d, testing_set_d, 'd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "\n",
    "1. Application of SVM classifier with TF•IDF is described in \n",
    "   [David Batista](http://www.davidsbatista.net/blog/2017/04/01/document_classification/) blog post.\n",
    "1. [Fasttext](https://fasttext.cc/) is a popular basline classifier. Don't report the Precision/Recall/F1 provided by\n",
    "   Fasttext since they might be [wrong](https://github.com/facebookresearch/fastText/issues/261).\n",
    "1. [Flair](https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f) \n",
    "   is another library for text processing. Flair classification is based on a language model.\n",
    "1. [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) by Jurafsky and Martin \n",
    "   has a [chapter](https://web.stanford.edu/~jurafsky/slp3/4.pdf) devoted to the problem of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
